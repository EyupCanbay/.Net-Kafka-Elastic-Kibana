# ğŸš€ Kafka & Elasticsearch HTTP Log Sistemi

Bu proje, gelen HTTP isteklerini otomatik olarak Kafka'ya gÃ¶nderen ve Elasticsearch'te saklayan event-driven bir loglama sistemidir.


---

## ğŸ— Sistem Mimarisi
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   HTTP      â”‚      â”‚   Kafka      â”‚      â”‚  Consumer   â”‚      â”‚ Elasticsearchâ”‚
â”‚   Request   â”‚â”€â”€â”€â”€â–¶â”‚   Topics      â”‚â”€â”€â”€â”€â–¶â”‚  Service    â”‚â”€â”€â”€â”€â–¶â”‚    Inde x    â”‚ 
â”‚             â”‚      â”‚              â”‚      â”‚             â”‚      â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   Kafka UI   â”‚
                     â”‚  (Port 8080) â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### BileÅŸenler

1. **Producer API (ASP.NET Core)**: HTTP isteklerini Middleware ile yakalayÄ±p Kafka'ya gÃ¶nderir
2. **Kafka Cluster (3 Broker)**: LoglarÄ± topic'lere gÃ¶re daÄŸÄ±tÄ±r ve saklar
3. **Consumer Service (C# Console App)**: Kafka'dan mesajlarÄ± okuyup Elasticsearch'e yazar
4. **Elasticsearch**: LoglarÄ± index'ler ve sorgulama imkanÄ± sunar
5. **Kibana**: Elasticsearch verilerini gÃ¶rselleÅŸtirir

**NOT**: Elasticsearch ve Kibana versiyonlarÄ±nÄ± aynÄ± yapÄ±lmalÄ±dÄ±r. Aksi halde hata alÄ±nÄ±yor.
---



## ğŸ›  Kurulum AdÄ±mlarÄ±

### ConfigÃ¼rasyonlarÄ±
```bash
compose dosyasÄ±nÄ± oluÅŸturun ve projedeki compose dosyasÄ±nÄ± kopyalayÄ±n ihtiyacÄ±nÄ±za gÃ¶re configure edebilirsiniz. 

Kafka Broker AyarlarÄ± (Kraft Modu)
KAFKA_NODE_ID: Cluster iÃ§indeki her broker'Ä±n benzersiz kimliÄŸidir (1, 2, 3).
KAFKA_PROCESS_ROLES: broker,controller. Kraft modunda olduÄŸu iÃ§in bu node hem veri taÅŸÄ±yÄ±cÄ± (broker) hem de yÃ¶netici (controller) rolÃ¼ndedir.
KAFKA_CONTROLLER_QUORUM_VOTERS: Lider seÃ§imi (voting) yapacak node'larÄ±n listesidir. Zookeeper olmadÄ±ÄŸÄ± iÃ§in cluster yÃ¶netimini bu node'lar kendi aralarÄ±nda yapar.
KAFKA_LISTENERS: Kafka'nÄ±n hangi protokolleri hangi portlardan dinleyeceÄŸini belirtir.
CONTROLLER: Cluster iÃ§i yÃ¶netim iletiÅŸimi.
PLAINTEXT: Docker network iÃ§indeki diÄŸer container'lar iÃ§in.
PLAINTEXT_HOST: DÄ±ÅŸ dÃ¼nyadan (bizim bilgisayarÄ±mÄ±zdan) eriÅŸim iÃ§in.
KAFKA_ADVERTISED_LISTENERS: Client'lara (Producer/Consumer) "Bana ulaÅŸmak iÃ§in bu adresi kullan" dediÄŸi kÄ±sÄ±mdÄ±r.
Docker iÃ§indeki servisler (Kafka UI) kafka-1:29092 adresini kullanÄ±r.
BilgisayarÄ±mÄ±zdaki .NET uygulamalarÄ± localhost:9092 adresini kullanÄ±r.
CLUSTER_ID: TÃ¼m node'larÄ±n aynÄ± cluster'a ait olduÄŸunu doÄŸrulayan benzersiz kimlik anahtarÄ±dÄ±r.
Elasticsearch AyarlarÄ±
discovery.type=single-node: Cluster oluÅŸturmadan tek bir node olarak Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlar (Local development iÃ§in).
xpack.security.enabled=false: GeliÅŸtirme ortamÄ± olduÄŸu iÃ§in kullanÄ±cÄ± adÄ±/ÅŸifre ve HTTPS zorunluluÄŸunu kapatÄ±r.

```

**Beklenilen Ã‡Ä±ktÄ±**: 6 container Ã§alÄ±ÅŸÄ±yor olmalÄ±:
- Kafka Cluster: 3 Broker (localhost:9092, 9093, 9094)
- Kafka UI: http://localhost:8080 (Cluster yÃ¶netimi iÃ§in)
- Elasticsearch: http://localhost:9200 (Log veritabanÄ±)
- Kibana: http://localhost:5601 (Log gÃ¶rselleÅŸtirme)

### Kafka Topic'lerinin OluÅŸturulmasÄ±

Topic'ler otomatik olarak **Producer API baÅŸlatÄ±ldÄ±ÄŸÄ±nda** oluÅŸturulur. AÅŸÅŸaÄŸÄ±daki kod bloÄŸu sayesinde
```bash
public class TopicInitializer
{
    public static async Task InitTopics(string bootstrapServers)
    {
        using var adminClient = new AdminClientBuilder(new AdminClientConfig { BootstrapServers = bootstrapServers }).Build();

        var topics = new[] { "http-200", "http-300", "http-400", "http-404", "http-500" };

        foreach (var topicName in topics)
        {
            try
            {
                // Partition ve Replication Factor ayarlarÄ±
                await adminClient.CreateTopicsAsync(new[]
                {
                    new TopicSpecification { Name = topicName, NumPartitions = 3, ReplicationFactor = 3 }
                });
                Console.WriteLine($"Topic OluÅŸturuldu: {topicName}");
            }
            // Topic zaten varsa sistem kendini kapatmasÄ±n diye.
            catch (CreateTopicsException e) when (e.Results[0].Error.Code == ErrorCode.TopicAlreadyExists) 
            {
                Console.WriteLine($"â„¹Topic mevcut: {topicName}");
            }
        }
    }
}

```

Ancak manuel oluÅŸturmak isterseniz:
```bash
# Container iÃ§ine gir
docker exec -it kafka-1 bash

# Topic oluÅŸtur
kafka-topics --create --topic http-200 --bootstrap-server localhost:29092 --partitions 3 --replication-factor 3
kafka-topics --create --topic http-300 --bootstrap-server localhost:29092 --partitions 3 --replication-factor 3
kafka-topics --create --topic http-400 --bootstrap-server localhost:29092 --partitions 3 --replication-factor 3
kafka-topics --create --topic http-404 --bootstrap-server localhost:29092 --partitions 3 --replication-factor 3
kafka-topics --create --topic http-500 --bootstrap-server localhost:29092 --partitions 3 --replication-factor 3

# Topic'leri listele
kafka-topics --list --bootstrap-server localhost:29092
```

### 4ï¸âƒ£ Consumer Service'i BaÅŸlatma
Consumer servisinde aÅŸÅŸaÄŸÄ±daki kod bloÄŸu ile kafkaya baÄŸlanÄ±lÄ±r ve belirtilen grupid'si ile birden fazla consumerÄ± kafkaya aynÄ± consumer olduÄŸunu sÃ¶yleriz. configurasyonlarÄ±mÄ±zÄ± yaparÄ±z.
```bash
        var config = new ConsumerConfig
        {
            BootstrapServers = "localhost:9092,localhost:9093,localhost:9094", - Kafka clusterâ€™a baÄŸlanmak iÃ§in broker adreslerini veriyoruz.
            GroupId = "log-processor-group-elastic", - Consumerâ€™Ä±n baÄŸlÄ± olduÄŸu consumer groupâ€™u belirtiyor.
            AutoOffsetReset = AutoOffsetReset.Earliest,  - Daha Ã¶nce okumadÄ±ÄŸÄ±n bir partition'a giriyorsan nereden baÅŸlayacaÄŸÄ±nÄ± belirler.
            EnableAutoCommit = false - Kafkaâ€™ya â€œmesajÄ± okudumâ€ bilgisinin otomatik gÃ¶nderilmesini kapatÄ±r.
        };

        using var consumer = new ConsumerBuilder<string, string>(config).Build();
        consumer.Subscribe(new[] { "http-200", "http-300", "http-400", "http-404", "http-500" }); topiclere baÄŸlÄ±yoruz

        Console.WriteLine("Consumer dinlemeye baÅŸladÄ±...");
```

---

## ğŸ“‚ Proje YapÄ±sÄ±
```
project-root/
â”‚
â”œâ”€â”€ docker-compose.yml              # TÃ¼m altyapÄ± tanÄ±mlarÄ± (Kafka, Elastic, Kibana)
â”‚
â”œâ”€â”€ KafkaProducerApi/               # HTTP isteklerini yakalayan API
â”‚   â”œâ”€â”€ Program.cs                  # Middleware ve endpoint tanÄ±mlarÄ±
â”‚   â”œâ”€â”€ Middleware/
â”‚   â”‚   â””â”€â”€ RequestLoggerMiddleware.cs  # Her isteÄŸi Kafka'ya gÃ¶nderen logic
â”‚   â”œâ”€â”€ Services/
â”‚   â”‚   â””â”€â”€ IKafkaProducerService.cs    # Kafka Producer wrapper
â”‚   â”œâ”€â”€ Helpers/
â”‚   â”‚   â””â”€â”€ TopicInitializer.cs         # Kafka topic'lerini otomatik oluÅŸturur
â”‚   â””â”€â”€ Models/
â”‚       â””â”€â”€ HttpLogModel.cs             # Log verisi iÃ§in DTO
â”‚
â””â”€â”€ KafkaConsumerService/           # Kafka'dan okuyan, Elastic'e yazan servis
    â”œâ”€â”€ Program.cs                  # Consumer logic + Elastic entegrasyonu
    â””â”€â”€ Models/
        â””â”€â”€ ElasticLogDocument.cs   # Elasticsearch dÃ¶kÃ¼man modeli
```

---

## âš™ï¸ Ã‡alÄ±ÅŸma MantÄ±ÄŸÄ±

### 1. HTTP Ä°stek YakalanmasÄ± (Middleware)

`RequestLoggerMiddleware` her HTTP isteÄŸi iÃ§in:
```csharp
// Ä°stek baÅŸÄ±nda sÃ¼reyi baÅŸlat
var stopwatch = Stopwatch.StartNew();

// Ä°steÄŸi iÅŸle
await _next(context);

// SÃ¼reyi durdur
stopwatch.Stop();

// Status code'a gÃ¶re topic belirle
string topicName = DetermineTopicName(statusCode);

// Kafka'ya gÃ¶nder
await producer.ProduceAsync(topicName, logData);
```

### 2. Topic YÃ¶nlendirme MantÄ±ÄŸÄ±

- **200-299 (BaÅŸarÄ±lÄ±)**: `http-200`, `http-300`, `http-400` arasÄ±ndan **RASTGELE** seÃ§ilir
- **300-399 (YÃ¶nlendirme)**: `http-300`
- **404 (BulunamadÄ±)**: `http-404`
- **400-499 (Ä°stemci HatasÄ±)**: `http-400`
- **500+ (Sunucu HatasÄ±)**: `http-500`

> **Neden Rastgele?** Load balancing test senaryolarÄ± iÃ§in farklÄ± topic'lere daÄŸÄ±tÄ±m saÄŸlanÄ±r.

### 3. Kafka'da Saklama

Her topic **3 partition** ve **3 replication factor** ile oluÅŸturulur:

- **Partition**: Paralel iÅŸleme ve yÃ¼k daÄŸÄ±lÄ±mÄ±
- **Replication**: Veri kaybÄ±na karÅŸÄ± yedekleme (1 broker Ã§Ã¶kse bile data kaybolmaz)

### 4. Consumer'Ä±n Elasticsearch'e YazmasÄ±
```csharp
// Kafka'dan oku
var result = consumer.Consume(TimeSpan.FromSeconds(1));

// JSON deserialize
var logData = JsonSerializer.Deserialize<HttpLogModel>(result.Message.Value, jsonOptions);

// Elasticsearch document oluÅŸtur
var elasticDoc = new ElasticLogDocument { ... };

// Index'e yaz
await elasticClient.IndexAsync(elasticDoc, documentId);

// Offset commit et (mesaj baÅŸarÄ±yla iÅŸlendi)
consumer.Commit(result);
```

### 5. Elasticsearch'te Indexleme

TÃ¼m loglar `http-logs-index` adlÄ± index'te saklanÄ±r:
```json
{
  "traceId": "0HMVUQ2...",
  "httpMethod": "GET",
  "path": "/api/success",
  "statusCode": 200,
  "durationMs": 45,
  "timestamp": "2025-12-01T10:30:00Z",
  "kafkaTopic": "http-200",
  "kafkaOffset": 42
}
```

---

## ğŸ§ª Test & DoÄŸrulama

### 1. Producer'Ä± Test Etme
```bash
# BaÅŸarÄ±lÄ± istek (200) - Rastgele topic'e gider
curl http://localhost:5000/api/success

# 404 hatasÄ±
curl http://localhost:5000/api/not-found

# 500 hatasÄ±
curl http://localhost:5000/api/server-error
```

### 2. Kafka UI ile Kontrol

TarayÄ±cÄ±da [http://localhost:8080](http://localhost:8080) aÃ§Ä±n:

1. **Topics** sekmesine tÄ±klayÄ±n
2. `http-200` gibi bir topic seÃ§in
3. **Messages** bÃ¶lÃ¼mÃ¼nde gÃ¶nderilen loglarÄ± gÃ¶rÃ¼n


### 3. Elasticsearch'te DoÄŸrulama
```bash
# TÃ¼m loglarÄ± listele
curl http://localhost:9200/http-logs-index/_search?pretty

# Son 10 logu getir
curl -X GET "http://localhost:9200/http-logs-index/_search?pretty" -H 'Content-Type: application/json' -d'
{
  "size": 10,
  "sort": [{"timestamp": "desc"}]
}
'
```

### 4. Kibana ile GÃ¶rselleÅŸtirme

1. [http://localhost:5601](http://localhost:5601) adresini aÃ§Ä±n
2. Sol menÃ¼den **Discover** seÃ§in
3. Index pattern oluÅŸturun: `http-logs-index*`
4. Timestamp alanÄ±nÄ± seÃ§in: `timestamp`
5. LoglarÄ± gerÃ§ek zamanlÄ± gÃ¶rÃ¼n ve filtreleyin

---



## ğŸ“Š Performans Metrikleri

- **Kafka Throughput**: ~10,000 mesaj/saniye (3 broker cluster)
- **Elasticsearch Indexing**: ~5,000 dÃ¶kÃ¼man/saniye
- **Ortalama Latency**: 50-100ms (Producer â†’ Elasticsearch)

---

## ğŸ”’ GÃ¼venlik NotlarÄ±

> âš ï¸ **DÄ°KKAT**: Bu yapÄ±landÄ±rma **sadece development ortamÄ±** iÃ§indir!

Production ortamÄ±nda mutlaka yapÄ±lmalÄ±:

1. **Elasticsearch Security** aktif edilmeli (`xpack.security.enabled=true`)
2. **Kafka SASL/SSL** ile ÅŸifrelenmeli
3. **API Gateway** kullanÄ±lmalÄ±
4. **Secrets Management** (Ã¶rn. Azure Key Vault, HashiCorp Vault)
5. **Network Isolation** (VPC/VNET)

---

